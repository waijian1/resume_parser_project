{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8089e14e-21b9-4dfd-84af-97d0519452b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "050cfe47-eecd-4693-8650-4592f4e2b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging if not already done\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e31a2b05-0cb4-449c-af4d-e9f5f8fdf866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "S3_BUCKET_NAME = \"resume-parser-waijian-20250525\"\n",
    "# Let's target one specific folder first\n",
    "S3_PREFIX = \"resume/ACCOUNTANT/\"\n",
    "TEXTRACT_ROLE_ARN = 'arn:aws:iam::747549824523:role/TextractS3AccessRole'\n",
    "AWS_REGION = \"ap-southeast-1\"\n",
    "# --- END CONFIGURATION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9df8885-824f-4bec-a322-9513a80dcbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:54:48,480 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-05-26 23:54:48,736 - INFO - Setup complete. Bucket: resume-parser-waijian-20250525, Prefix: resume/ACCOUNTANT/\n"
     ]
    }
   ],
   "source": [
    "# Create Boto3 clients\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "textract_client = boto3.client('textract', region_name=AWS_REGION)\n",
    "\n",
    "logging.info(f\"Setup complete. Bucket: {S3_BUCKET_NAME}, Prefix: {S3_PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91377d4f-5e0d-4b9e-a4d7-a1e29f8eac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:54:51,218 - INFO - Found 5 PDFs in S3 under resume/ACCOUNTANT/ (showing max 5).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process:\n",
      "['resume/ACCOUNTANT/10554236.pdf', 'resume/ACCOUNTANT/10674770.pdf', 'resume/ACCOUNTANT/11163645.pdf', 'resume/ACCOUNTANT/11759079.pdf', 'resume/ACCOUNTANT/12065211.pdf']\n"
     ]
    }
   ],
   "source": [
    "def list_s3_files(bucket, prefix, max_keys=10):\n",
    "    \"\"\"Lists files in an S3 bucket under a given prefix.\"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=max_keys)\n",
    "        files = [item['Key'] for item in response.get('Contents', []) if item['Key'].lower().endswith('.pdf')]\n",
    "        logging.info(f\"Found {len(files)} PDFs in S3 under {prefix} (showing max {max_keys}).\")\n",
    "        return files\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error listing S3 files: {e}\")\n",
    "        return []\n",
    "\n",
    "pdf_files_to_process = list_s3_files(S3_BUCKET_NAME, S3_PREFIX, max_keys=5) # Let's start with 5\n",
    "print(\"Files to process:\")\n",
    "print(pdf_files_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4a91e7-16e6-4392-9d0f-863a2243f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:54:53,997 - INFO - Starting Textract job for: resume/ACCOUNTANT/10554236.pdf\n",
      "2025-05-26 23:54:54,463 - INFO - Started Job with ID: c37f258c97e986fe107fa8ae5b44a3143a86f3055c27f79fa44d13308b761541\n"
     ]
    }
   ],
   "source": [
    "def start_textract_job(bucket, document_key):\n",
    "    \"\"\"Starts an asynchronous Textract job.\"\"\"\n",
    "    if not document_key:\n",
    "        logging.error(\"No document key provided.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Starting Textract job for: {document_key}\")\n",
    "    try:\n",
    "        response = textract_client.start_document_text_detection(\n",
    "            DocumentLocation={\n",
    "                'S3Object': {\n",
    "                    'Bucket': bucket,\n",
    "                    'Name': document_key\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        job_id = response['JobId']\n",
    "        logging.info(f\"Started Job with ID: {job_id}\")\n",
    "        return job_id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error starting Textract job for {document_key}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Start a job for the *first* PDF in our list (if any)\n",
    "current_job_id = None\n",
    "if pdf_files_to_process:\n",
    "    first_pdf = pdf_files_to_process[0]\n",
    "    current_job_id = start_textract_job(S3_BUCKET_NAME, first_pdf)\n",
    "else:\n",
    "    logging.warning(\"No PDFs found to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5738fca-e243-4519-b30f-4d57f57ca3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:54:56,889 - INFO - Checking status for Job ID: c37f258c97e986fe107fa8ae5b44a3143a86f3055c27f79fa44d13308b761541\n",
      "2025-05-26 23:54:56,904 - INFO - Current Status: IN_PROGRESS\n",
      "2025-05-26 23:54:56,905 - INFO - Waiting for 5 seconds...\n",
      "2025-05-26 23:55:01,910 - INFO - Checking status for Job ID: c37f258c97e986fe107fa8ae5b44a3143a86f3055c27f79fa44d13308b761541\n",
      "2025-05-26 23:55:02,152 - INFO - Current Status: SUCCEEDED\n",
      "2025-05-26 23:55:02,153 - INFO - Job c37f258c97e986fe107fa8ae5b44a3143a86f3055c27f79fa44d13308b761541 SUCCEEDED!\n"
     ]
    }
   ],
   "source": [
    "def check_textract_job_status(job_id):\n",
    "    \"\"\"Checks the status of a Textract job.\"\"\"\n",
    "    if not job_id:\n",
    "        logging.error(\"No Job ID provided.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Checking status for Job ID: {job_id}\")\n",
    "    try:\n",
    "        response = textract_client.get_document_text_detection(JobId=job_id)\n",
    "        status = response['JobStatus']\n",
    "        logging.info(f\"Current Status: {status}\")\n",
    "        return status\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error checking job status {job_id}: {e}\")\n",
    "        return \"FAILED\" # Treat errors as failures\n",
    "\n",
    "def wait_for_job_completion(job_id, delay=5, timeout=300):\n",
    "    \"\"\"Waits for a Textract job to complete by polling.\"\"\"\n",
    "    if not job_id: return False\n",
    "\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        status = check_textract_job_status(job_id)\n",
    "        if status == 'SUCCEEDED':\n",
    "            logging.info(f\"Job {job_id} SUCCEEDED!\")\n",
    "            return True\n",
    "        elif status in ['FAILED', 'PARTIAL_SUCCESS']:\n",
    "            logging.error(f\"Job {job_id} finished with status: {status}\")\n",
    "            return False\n",
    "        \n",
    "        logging.info(f\"Waiting for {delay} seconds...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    logging.error(f\"Job {job_id} timed out after {timeout} seconds.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "# Wait for the job we started (if any)\n",
    "job_succeeded = False\n",
    "if current_job_id:\n",
    "    job_succeeded = wait_for_job_completion(current_job_id)\n",
    "else:\n",
    "    logging.warning(\"No job was started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bfc8ec3-9fc3-4e9a-b730-057a75cf7511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:55:07,556 - INFO - Retrieving results for Job ID: c37f258c97e986fe107fa8ae5b44a3143a86f3055c27f79fa44d13308b761541\n",
      "2025-05-26 23:55:08,339 - INFO - Retrieved 3732 blocks for Job ID: c37f258c97e986fe107fa8ae5b44a3143a86f3055c27f79fa44d13308b761541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EXTRACTED TEXT (First 1000 chars) ---\n",
      "ACCOUNTANT\n",
      "Summary\n",
      "Financial Accountant specializing in financial planning, reporting and analysis within the Department of Defense.\n",
      "Highlights\n",
      "Account reconciliations\n",
      "Results-oriented\n",
      "Accounting operations professional\n",
      "Financial reporting\n",
      "Analysis of financial systems\n",
      "Critical thinking\n",
      "ERP (Enterprise Resource Planning) software.\n",
      "Excellent facilitator\n",
      "Accomplishments\n",
      "Served on a tiger team which identified and resolved General Ledger postings in DEAMS totaling $360B in accounting adjustments. This allowed\n",
      "for the first successful fiscal year-end close for 2012.\n",
      "In collaboration with DFAS Europe, developed an automated tool that identified duplicate obligations. This tool allowed HQ USAFE to\n",
      "deobligate over $5M in duplicate obligations.\n",
      "Experience\n",
      "Company Name July 2011 to November 2012 Accountant\n",
      "City, State\n",
      "Enterprise Resource Planning Office (ERO)\n",
      "In this position as an Accountant assigned to the Defense Enterprise Accounting and Management System (DEAMS) ERO I was\n",
      "responsible for i\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_textract_results(job_id):\n",
    "    \"\"\"Retrieves all pages of results for a completed Textract job.\"\"\"\n",
    "    if not job_id: return None\n",
    "    \n",
    "    logging.info(f\"Retrieving results for Job ID: {job_id}\")\n",
    "    all_blocks = []\n",
    "    next_token = None\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            if next_token:\n",
    "                response = textract_client.get_document_text_detection(JobId=job_id, NextToken=next_token)\n",
    "            else:\n",
    "                response = textract_client.get_document_text_detection(JobId=job_id)\n",
    "            \n",
    "            blocks = response.get('Blocks', [])\n",
    "            all_blocks.extend(blocks)\n",
    "            \n",
    "            next_token = response.get('NextToken')\n",
    "            if not next_token:\n",
    "                break # No more pages\n",
    "                \n",
    "        logging.info(f\"Retrieved {len(all_blocks)} blocks for Job ID: {job_id}\")\n",
    "        return all_blocks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving results for {job_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_blocks(blocks):\n",
    "    \"\"\"Extracts plain text (lines) from Textract blocks.\"\"\"\n",
    "    if not blocks: return \"\"\n",
    "    \n",
    "    lines = []\n",
    "    for block in blocks:\n",
    "        if block['BlockType'] == 'LINE':\n",
    "            lines.append(block['Text'])\n",
    "            \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Get and process results if the job succeeded\n",
    "if job_succeeded and current_job_id:\n",
    "    textract_blocks = get_textract_results(current_job_id)\n",
    "    \n",
    "    if textract_blocks:\n",
    "        # Optional: Save the full JSON for inspection\n",
    "        # with open(f'{current_job_id}.json', 'w') as f:\n",
    "        #     json.dump(textract_blocks, f, indent=4)\n",
    "        # logging.info(f\"Saved full JSON to {current_job_id}.json\")\n",
    "\n",
    "        # Extract and print the plain text\n",
    "        extracted_text = extract_text_from_blocks(textract_blocks)\n",
    "        print(\"\\n--- EXTRACTED TEXT (First 1000 chars) ---\")\n",
    "        print(extracted_text[:1000])\n",
    "        print(\"----------------------------------------\")\n",
    "        \n",
    "    else:\n",
    "        logging.error(\"Failed to retrieve Textract blocks.\")\n",
    "else:\n",
    "    logging.warning(\"Job did not succeed or no job was run. Cannot get results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef3f1f0-5505-45ce-ac77-85c26c86cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 00:26:15,355 - INFO - Calling AWS Comprehend DetectEntities...\n",
      "2025-05-27 00:26:15,506 - INFO - Comprehend found 64 entities.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Found Entities (AWS Comprehend) ---\n",
      "  - Text: Department of Defense, Type: ORGANIZATION, Score: 0.98\n",
      "  - Text: DEAMS, Type: ORGANIZATION, Score: 0.63\n",
      "  - Text: $360B, Type: QUANTITY, Score: 1.00\n",
      "  - Text: first, Type: QUANTITY, Score: 0.86\n",
      "  - Text: fiscal year-end, Type: DATE, Score: 0.73\n",
      "  - Text: 2012, Type: DATE, Score: 0.97\n",
      "  - Text: DFAS Europe, Type: ORGANIZATION, Score: 0.99\n",
      "  - Text: HQ, Type: ORGANIZATION, Score: 0.86\n",
      "  - Text: USAFE, Type: ORGANIZATION, Score: 0.79\n",
      "  - Text: over $5M, Type: QUANTITY, Score: 0.97\n",
      "  - Text: July 2011, Type: DATE, Score: 1.00\n",
      "  - Text: November 2012, Type: DATE, Score: 1.00\n",
      "  - Text: State\n",
      "Enterprise Resource Planning Office, Type: ORGANIZATION, Score: 0.84\n",
      "  - Text: ERO, Type: ORGANIZATION, Score: 0.82\n",
      "  - Text: Defense Enterprise Accounting and Management System, Type: ORGANIZATION, Score: 0.93\n",
      "  - Text: DEAMS, Type: ORGANIZATION, Score: 0.77\n",
      "  - Text: ERO, Type: ORGANIZATION, Score: 0.60\n",
      "  - Text: DEAMS, Type: ORGANIZATION, Score: 0.80\n",
      "  - Text: DEAMS, Type: ORGANIZATION, Score: 0.88\n",
      "  - Text: DEAMS, Type: ORGANIZATION, Score: 0.99\n",
      "  - Text: DEAMS Program Management Office, Type: ORGANIZATION, Score: 0.62\n",
      "  - Text: $360B, Type: QUANTITY, Score: 1.00\n",
      "  - Text: first, Type: QUANTITY, Score: 0.93\n",
      "  - Text: fiscal year, Type: DATE, Score: 0.60\n",
      "  - Text: end, Type: DATE, Score: 0.59\n",
      "  - Text: 2012, Type: DATE, Score: 0.99\n",
      "  - Text: fiscal year 2010, Type: DATE, Score: 0.93\n",
      "  - Text: 2011, Type: DATE, Score: 1.00\n",
      "  - Text: DEAMS, Type: ORGANIZATION, Score: 0.97\n",
      "  - Text: Air Force Operational Test and Evaluation Center, Type: ORGANIZATION, Score: 1.00\n",
      "  - Text: AFOTEC, Type: ORGANIZATION, Score: 0.99\n",
      "  - Text: DEAMS, Type: ORGANIZATION, Score: 0.96\n",
      "  - Text: April 2010, Type: DATE, Score: 1.00\n",
      "  - Text: June 2011, Type: DATE, Score: 1.00\n",
      "  - Text: 1st Air Communications Operation Squadron, Type: ORGANIZATION, Score: 0.98\n",
      "  - Text: 1ACOS, Type: ORGANIZATION, Score: 0.99\n",
      "  - Text: $4.6M, Type: QUANTITY, Score: 1.00\n",
      "  - Text: USAFE Directorate of Intelligence, Type: ORGANIZATION, Score: 0.98\n",
      "  - Text: USAFE, Type: ORGANIZATION, Score: 1.00\n",
      "  - Text: A2, Type: ORGANIZATION, Score: 0.66\n",
      "  - Text: USAFE Directorate of Air and Space Operations, Type: ORGANIZATION, Score: 0.99\n",
      "  - Text: USAFE/A3, Type: ORGANIZATION, Score: 0.70\n",
      "  - Text: USAFE Directorate of Communications, Type: ORGANIZATION, Score: 0.99\n",
      "  - Text: USAFE/A6, Type: ORGANIZATION, Score: 0.82\n",
      "  - Text: 435th Air Ground Operations Wing, Type: ORGANIZATION, Score: 0.82\n",
      "  - Text: 1ACOS, Type: ORGANIZATION, Score: 1.00\n",
      "  - Text: three, Type: QUANTITY, Score: 0.58\n",
      "  - Text: each organization, Type: QUANTITY, Score: 0.98\n",
      "  - Text: CC, Type: ORGANIZATION, Score: 0.59\n",
      "  - Text: 1ACOS, Type: ORGANIZATION, Score: 0.52\n",
      "  - Text: Defense Travel System, Type: ORGANIZATION, Score: 0.82\n",
      "  - Text: DTS, Type: ORGANIZATION, Score: 0.71\n",
      "  - Text: DTS, Type: ORGANIZATION, Score: 0.99\n",
      "  - Text: General Accounting and Finance System, Type: ORGANIZATION, Score: 0.97\n",
      "  - Text: GPC, Type: ORGANIZATION, Score: 0.85\n",
      "  - Text: Control Program, Type: OTHER, Score: 0.52\n",
      "  - Text: zero findings, Type: QUANTITY, Score: 0.92\n",
      "  - Text: 700th, Type: QUANTITY, Score: 0.47\n",
      "  - Text: CONS, Type: ORGANIZATION, Score: 0.54\n",
      "  - Text: zero findings, Type: QUANTITY, Score: 0.96\n",
      "  - Text: July 2008, Type: DATE, Score: 1.00\n",
      "  - Text: April 2010, Type: DATE, Score: 1.00\n",
      "  - Text: USAFE, Type: ORGANIZATION, Score: 1.00\n",
      "  - Text: USAFE, Type: ORGANIZATION, Score: 1.00\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a Boto3 client for Comprehend\n",
    "comprehend_client = boto3.client('comprehend', region_name=AWS_REGION)\n",
    "\n",
    "def find_entities_comprehend(text):\n",
    "    \"\"\"Uses AWS Comprehend to find named entities.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Comprehend has a 5000 byte limit per call.\n",
    "    # We should handle larger texts, but for a start, let's truncate.\n",
    "    # A better approach would be to split the text.\n",
    "    text_to_process = text[:4900] # Stay safely under the limit\n",
    "\n",
    "    logging.info(\"Calling AWS Comprehend DetectEntities...\")\n",
    "    try:\n",
    "        response = comprehend_client.detect_entities(\n",
    "            Text=text_to_process,\n",
    "            LanguageCode='en' # Assuming English resumes\n",
    "        )\n",
    "        entities = response.get('Entities', [])\n",
    "        logging.info(f\"Comprehend found {len(entities)} entities.\")\n",
    "        return entities\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calling AWS Comprehend: {e}\")\n",
    "        return []\n",
    "\n",
    "# Find entities using Comprehend\n",
    "if 'extracted_text' in locals():\n",
    "    comprehend_entities = find_entities_comprehend(extracted_text)\n",
    "    print(\"\\n--- Found Entities (AWS Comprehend) ---\")\n",
    "    # Print a summary: Text, Type, Score\n",
    "    for entity in comprehend_entities:\n",
    "        print(f\"  - Text: {entity['Text']}, Type: {entity['Type']}, Score: {entity['Score']:.2f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "else:\n",
    "    print(\"Cannot perform Comprehend search: 'extracted_text' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b663752-79f4-4823-8f3b-43b8f3ddbdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILL_KEYWORDS = [\n",
    "    # Programming Languages\n",
    "    'python', 'java', 'c++', 'c#', 'javascript', 'typescript', 'php', 'ruby', 'go', 'swift', 'kotlin', 'sql', 'pl/sql', 'scala',\n",
    "    # Web Frameworks/Libraries\n",
    "    'react', 'angular', 'vue', 'django', 'flask', 'spring', 'node.js', 'jquery', 'bootstrap', '.net',\n",
    "    # Databases\n",
    "    'mysql', 'postgresql', 'mongodb', 'redis', 'oracle', 'sql server', 'sqlite', 'cassandra', 'dynamodb',\n",
    "    # Cloud/AWS\n",
    "    'aws', 'azure', 'gcp', 'amazon web services', 'google cloud platform', 's3', 'ec2', 'lambda', 'rds', 'eks', 'ecs', 'textract', 'comprehend', 'sagemaker',\n",
    "    # ML/Data Science\n",
    "    'machine learning', 'data science', 'deep learning', 'nlp', 'natural language processing', 'pandas', 'numpy', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'matplotlib', 'seaborn', 'spark', 'hadoop', 'airflow', 'mlflow',\n",
    "    # DevOps/Tools\n",
    "    'docker', 'kubernetes', 'jenkins', 'git', 'github', 'gitlab', 'ansible', 'terraform', 'ci/cd', 'jira',\n",
    "    # OS\n",
    "    'linux', 'windows', 'macos',\n",
    "    # Methodologies\n",
    "    'agile', 'scrum', 'kanban',\n",
    "    # Other\n",
    "    'api', 'rest', 'graphql', 'microservices', 'statistics', 'operations research', 'data analysis', 'etl', 'power bi', 'tableau'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f65ba3e-a6a4-4200-b982-bb37e77bbcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Found Skills (Rule-Based) ---\n",
      "['rest', 'aws', 'rds', 'api', 'go']\n",
      "Found 5 skills.\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "def find_skills_keyword_based(text, skills_list):\n",
    "    \"\"\"Finds skills from a list in the given text.\"\"\"\n",
    "    found_skills = set() # Use a set to avoid duplicates\n",
    "\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Convert text to lowercase for case-insensitive matching\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    for skill in skills_list:\n",
    "        # Check if the skill (as a whole word or phrase) is present.\n",
    "        # We can use simple 'in' for now, but regex (word boundaries) would be more robust.\n",
    "        if skill in text_lower:\n",
    "            # We add the original cased skill for better presentation if needed,\n",
    "            # but for consistency, let's add the lowercase version we searched for.\n",
    "            found_skills.add(skill)\n",
    "\n",
    "    return list(found_skills)\n",
    "\n",
    "# Find skills in our extracted text\n",
    "if 'extracted_text' in locals():\n",
    "    found_skills_rule_based = find_skills_keyword_based(extracted_text, SKILL_KEYWORDS)\n",
    "    print(\"\\n--- Found Skills (Rule-Based) ---\")\n",
    "    print(found_skills_rule_based)\n",
    "    print(f\"Found {len(found_skills_rule_based)} skills.\")\n",
    "    print(\"---------------------------------\")\n",
    "else:\n",
    "    print(\"Cannot perform rule-based search: 'extracted_text' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65f0bccc-8143-42fc-a8ea-a4621140295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined & Filtered Keywords/Entities ---\n",
      "['rest', 'aws', 'rds', 'control program', 'api', 'go']\n",
      "Found 6 unique combined keywords/entities.\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def combine_results(rule_skills, comp_entities, exclude_types=None):\n",
    "    \"\"\"\n",
    "    Combines skills from rule-based matching and entities from Comprehend,\n",
    "    optionally excluding certain Comprehend types.\n",
    "    \"\"\"\n",
    "    if exclude_types is None:\n",
    "        exclude_types = ['PERSON', 'LOCATION', 'DATE', 'ORGANIZATION', 'QUANTITY']\n",
    "\n",
    "    # Start with rule-based skills, converted to lowercase\n",
    "    combined_set = set([skill.lower() for skill in rule_skills])\n",
    "\n",
    "    # Process Comprehend entities\n",
    "    for entity in comp_entities:\n",
    "        # If the entity type is not in our exclusion list...\n",
    "        if entity['Type'] not in exclude_types:\n",
    "            # ...add its text (converted to lowercase) to the set.\n",
    "            combined_set.add(entity['Text'].lower())\n",
    "\n",
    "    return list(combined_set)\n",
    "\n",
    "# --- Ensure we have the results before combining ---\n",
    "# Make sure 'found_skills_rule_based' exists\n",
    "if 'found_skills_rule_based' not in locals():\n",
    "    found_skills_rule_based = []\n",
    "    print(\"Warning: 'found_skills_rule_based' not found, using empty list.\")\n",
    "\n",
    "# Make sure 'comprehend_entities' exists\n",
    "if 'comprehend_entities' not in locals():\n",
    "    comprehend_entities = []\n",
    "    print(\"Warning: 'comprehend_entities' not found, using empty list.\")\n",
    "# --- End Ensure ---\n",
    "\n",
    "\n",
    "# Perform the combination\n",
    "combined_keywords = combine_results(found_skills_rule_based, comprehend_entities)\n",
    "\n",
    "print(\"\\n--- Combined & Filtered Keywords/Entities ---\")\n",
    "print(combined_keywords)\n",
    "print(f\"Found {len(combined_keywords)} unique combined keywords/entities.\")\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# You might also want to keep the *full* Comprehend results for the UI,\n",
    "# but for a 'skill list', this 'combined_keywords' is a good start.\n",
    "# We can store the full results too:\n",
    "full_details = {\n",
    "    \"RuleBasedSkills\": found_skills_rule_based,\n",
    "    \"ComprehendEntities\": comprehend_entities,\n",
    "    \"CombinedKeywords\": combined_keywords,\n",
    "    \"RawText\": extracted_text[:2000] # Store a snippet or full text\n",
    "}\n",
    "\n",
    "# print(\"\\n--- Full Details (Sample) ---\")\n",
    "# print(json.dumps(full_details, indent=2, default=str)) # Use default=str for any non-serializable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4090c71-2910-4f19-940a-e41f34f4b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/27 00:49:26 INFO mlflow.tracking.fluent: Experiment with name 'Resume_Processing_Textract' does not exist. Creating a new experiment.\n",
      "2025-05-27 00:49:26,056 - INFO - MLflow tracking URI: file:///home/waijianlim/resume_parser_project/notebooks/mlruns\n",
      "2025-05-27 00:49:26,056 - INFO - MLflow experiment set to 'Resume_Processing_Textract'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import tempfile # We'll need this to save artifacts temporarily\n",
    "\n",
    "# Set an experiment name. If it doesn't exist, MLflow creates it.\n",
    "mlflow.set_experiment(\"Resume_Processing_Textract\")\n",
    "\n",
    "logging.info(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "logging.info(f\"MLflow experiment set to 'Resume_Processing_Textract'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e20c53a6-ed8e-48d0-b6f1-53909f4cb3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume_and_log(s3_bucket, s3_key, role_arn, skills_list, exclude_types):\n",
    "    \"\"\"\n",
    "    Processes a single resume PDF from S3 using Textract, extracts skills,\n",
    "    and logs parameters, metrics, and artifacts to MLflow.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Starting MLflow run for: {s3_key} ---\")\n",
    "\n",
    "    # Start an MLflow run. Everything inside 'with' gets logged to this run.\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        # 1. Log Parameters\n",
    "        logging.info(\"Logging parameters...\")\n",
    "        mlflow.log_param(\"s3_bucket\", s3_bucket)\n",
    "        mlflow.log_param(\"s3_key\", s3_key)\n",
    "        mlflow.log_param(\"textract_role_arn\", role_arn)\n",
    "        mlflow.log_param(\"comprehend_exclude_types\", str(exclude_types))\n",
    "        mlflow.log_param(\"num_skill_keywords\", len(skills_list))\n",
    "\n",
    "        # 2. Run Textract (Start & Poll)\n",
    "        job_id = start_textract_job(s3_bucket, s3_key)\n",
    "        if not job_id:\n",
    "            logging.error(\"Failed to start Textract job. Ending run.\")\n",
    "            mlflow.log_metric(\"status\", 0) # Log 0 for failure\n",
    "            return False\n",
    "\n",
    "        mlflow.log_param(\"textract_job_id\", job_id)\n",
    "        job_succeeded = wait_for_job_completion(job_id)\n",
    "\n",
    "        if not job_succeeded:\n",
    "            logging.error(\"Textract job did not succeed. Ending run.\")\n",
    "            mlflow.log_metric(\"status\", 0)\n",
    "            return False\n",
    "\n",
    "        # 3. Get Textract Results & Extract Text\n",
    "        textract_blocks = get_textract_results(job_id)\n",
    "        if not textract_blocks:\n",
    "            logging.error(\"Failed to get Textract results. Ending run.\")\n",
    "            mlflow.log_metric(\"status\", 0)\n",
    "            return False\n",
    "\n",
    "        extracted_text = extract_text_from_blocks(textract_blocks)\n",
    "\n",
    "        # 4. Perform Skill Extraction (Rule-Based & Comprehend)\n",
    "        found_skills_rule_based = find_skills_keyword_based(extracted_text, skills_list)\n",
    "        comprehend_entities = find_entities_comprehend(extracted_text) # Assumes client exists\n",
    "        combined_keywords = combine_results(found_skills_rule_based, comprehend_entities, exclude_types)\n",
    "\n",
    "        # 5. Log Metrics\n",
    "        logging.info(\"Logging metrics...\")\n",
    "        mlflow.log_metric(\"text_length_chars\", len(extracted_text))\n",
    "        mlflow.log_metric(\"num_textract_blocks\", len(textract_blocks))\n",
    "        mlflow.log_metric(\"num_rule_based_skills\", len(found_skills_rule_based))\n",
    "        mlflow.log_metric(\"num_comprehend_entities\", len(comprehend_entities))\n",
    "        mlflow.log_metric(\"num_combined_keywords\", len(combined_keywords))\n",
    "        mlflow.log_metric(\"status\", 1) # Log 1 for success\n",
    "\n",
    "        # 6. Log Artifacts\n",
    "        logging.info(\"Logging artifacts...\")\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            # Save extracted text\n",
    "            text_path = os.path.join(tmpdir, \"extracted_text.txt\")\n",
    "            with open(text_path, \"w\", encoding='utf-8') as f:\n",
    "                f.write(extracted_text)\n",
    "            mlflow.log_artifact(text_path, \"extracted_content\")\n",
    "\n",
    "            # Save combined keywords\n",
    "            keywords_path = os.path.join(tmpdir, \"combined_keywords.json\")\n",
    "            with open(keywords_path, \"w\") as f:\n",
    "                json.dump(combined_keywords, f, indent=4)\n",
    "            mlflow.log_artifact(keywords_path, \"extracted_content\")\n",
    "\n",
    "            # Save full Textract JSON (can be large!)\n",
    "            # textract_path = os.path.join(tmpdir, \"textract_output.json\")\n",
    "            # with open(textract_path, \"w\") as f:\n",
    "            #     json.dump(textract_blocks, f, indent=4)\n",
    "            # mlflow.log_artifact(textract_path, \"raw_outputs\")\n",
    "\n",
    "            # Save skill list used\n",
    "            skill_list_path = os.path.join(tmpdir, \"skill_keywords_used.json\")\n",
    "            with open(skill_list_path, \"w\") as f:\n",
    "                json.dump(skills_list, f, indent=4)\n",
    "            mlflow.log_artifact(skill_list_path, \"parameters\")\n",
    "\n",
    "\n",
    "        logging.info(f\"--- MLflow run COMPLETED for: {s3_key} ---\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2adb7d6-6bc8-4508-8247-780c6a1ef3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 00:57:27,378 - INFO - \n",
      ">>> Processing first PDF...\n",
      "2025-05-27 00:57:27,379 - INFO - --- Starting MLflow run for: resume/ACCOUNTANT/10554236.pdf ---\n",
      "2025-05-27 00:57:27,459 - INFO - Logging parameters...\n",
      "2025-05-27 00:57:27,462 - INFO - Starting Textract job for: resume/ACCOUNTANT/10554236.pdf\n",
      "2025-05-27 00:57:27,659 - INFO - Started Job with ID: 68dd7d3a6aec945de917cb0823bbca83cd3e5a9695a059b04a1efa3c955e6459\n",
      "2025-05-27 00:57:27,661 - INFO - Checking status for Job ID: 68dd7d3a6aec945de917cb0823bbca83cd3e5a9695a059b04a1efa3c955e6459\n",
      "2025-05-27 00:57:27,676 - INFO - Current Status: IN_PROGRESS\n",
      "2025-05-27 00:57:27,677 - INFO - Waiting for 5 seconds...\n",
      "2025-05-27 00:57:32,689 - INFO - Checking status for Job ID: 68dd7d3a6aec945de917cb0823bbca83cd3e5a9695a059b04a1efa3c955e6459\n",
      "2025-05-27 00:57:32,702 - INFO - Current Status: IN_PROGRESS\n",
      "2025-05-27 00:57:32,702 - INFO - Waiting for 5 seconds...\n",
      "2025-05-27 00:57:37,714 - INFO - Checking status for Job ID: 68dd7d3a6aec945de917cb0823bbca83cd3e5a9695a059b04a1efa3c955e6459\n",
      "2025-05-27 00:57:37,953 - INFO - Current Status: SUCCEEDED\n",
      "2025-05-27 00:57:37,954 - INFO - Job 68dd7d3a6aec945de917cb0823bbca83cd3e5a9695a059b04a1efa3c955e6459 SUCCEEDED!\n",
      "2025-05-27 00:57:37,954 - INFO - Retrieving results for Job ID: 68dd7d3a6aec945de917cb0823bbca83cd3e5a9695a059b04a1efa3c955e6459\n",
      "2025-05-27 00:57:38,656 - INFO - Retrieved 3732 blocks for Job ID: 68dd7d3a6aec945de917cb0823bbca83cd3e5a9695a059b04a1efa3c955e6459\n",
      "2025-05-27 00:57:38,658 - INFO - Calling AWS Comprehend DetectEntities...\n",
      "2025-05-27 00:57:38,832 - INFO - Comprehend found 64 entities.\n",
      "2025-05-27 00:57:38,833 - INFO - Logging metrics...\n",
      "2025-05-27 00:57:38,836 - INFO - Logging artifacts...\n",
      "2025-05-27 00:57:38,840 - INFO - --- MLflow run COMPLETED for: resume/ACCOUNTANT/10554236.pdf ---\n",
      "2025-05-27 00:57:38,844 - INFO - \n",
      ">>> Processing second PDF...\n",
      "2025-05-27 00:57:38,845 - INFO - --- Starting MLflow run for: resume/ACCOUNTANT/10674770.pdf ---\n",
      "2025-05-27 00:57:38,850 - INFO - Logging parameters...\n",
      "2025-05-27 00:57:38,853 - INFO - Starting Textract job for: resume/ACCOUNTANT/10674770.pdf\n",
      "2025-05-27 00:57:38,993 - INFO - Started Job with ID: 3e4cb051019fbadc489ff4ced1f38884a20da6138a48c295b76ca75417f10f10\n",
      "2025-05-27 00:57:38,995 - INFO - Checking status for Job ID: 3e4cb051019fbadc489ff4ced1f38884a20da6138a48c295b76ca75417f10f10\n",
      "2025-05-27 00:57:39,006 - INFO - Current Status: IN_PROGRESS\n",
      "2025-05-27 00:57:39,006 - INFO - Waiting for 5 seconds...\n",
      "2025-05-27 00:57:44,009 - INFO - Checking status for Job ID: 3e4cb051019fbadc489ff4ced1f38884a20da6138a48c295b76ca75417f10f10\n",
      "2025-05-27 00:57:44,066 - INFO - Current Status: IN_PROGRESS\n",
      "2025-05-27 00:57:44,066 - INFO - Waiting for 5 seconds...\n",
      "2025-05-27 00:57:49,070 - INFO - Checking status for Job ID: 3e4cb051019fbadc489ff4ced1f38884a20da6138a48c295b76ca75417f10f10\n",
      "2025-05-27 00:57:49,320 - INFO - Current Status: SUCCEEDED\n",
      "2025-05-27 00:57:49,321 - INFO - Job 3e4cb051019fbadc489ff4ced1f38884a20da6138a48c295b76ca75417f10f10 SUCCEEDED!\n",
      "2025-05-27 00:57:49,321 - INFO - Retrieving results for Job ID: 3e4cb051019fbadc489ff4ced1f38884a20da6138a48c295b76ca75417f10f10\n",
      "2025-05-27 00:57:49,571 - INFO - Retrieved 1135 blocks for Job ID: 3e4cb051019fbadc489ff4ced1f38884a20da6138a48c295b76ca75417f10f10\n",
      "2025-05-27 00:57:49,572 - INFO - Calling AWS Comprehend DetectEntities...\n",
      "2025-05-27 00:57:49,717 - INFO - Comprehend found 52 entities.\n",
      "2025-05-27 00:57:49,717 - INFO - Logging metrics...\n",
      "2025-05-27 00:57:49,721 - INFO - Logging artifacts...\n",
      "2025-05-27 00:57:49,723 - INFO - --- MLflow run COMPLETED for: resume/ACCOUNTANT/10674770.pdf ---\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have our clients and lists defined from previous sections\n",
    "# (s3_client, textract_client, comprehend_client, SKILL_KEYWORDS)\n",
    "\n",
    "# Define the types to exclude for Comprehend\n",
    "COMPREHEND_EXCLUDE = ['PERSON', 'LOCATION', 'DATE', 'ORGANIZATION', 'QUANTITY']\n",
    "\n",
    "# Process the first PDF (if available)\n",
    "if pdf_files_to_process:\n",
    "    logging.info(\"\\n>>> Processing first PDF...\")\n",
    "    process_resume_and_log(\n",
    "        S3_BUCKET_NAME,\n",
    "        pdf_files_to_process[0], # Process the first file\n",
    "        TEXTRACT_ROLE_ARN,\n",
    "        SKILL_KEYWORDS,\n",
    "        COMPREHEND_EXCLUDE\n",
    "    )\n",
    "\n",
    "# Process the second PDF (if available)\n",
    "if len(pdf_files_to_process) > 1:\n",
    "    logging.info(\"\\n>>> Processing second PDF...\")\n",
    "    process_resume_and_log(\n",
    "        S3_BUCKET_NAME,\n",
    "        pdf_files_to_process[1], # Process the second file\n",
    "        TEXTRACT_ROLE_ARN,\n",
    "        SKILL_KEYWORDS,\n",
    "        COMPREHEND_EXCLUDE\n",
    "    )\n",
    "else:\n",
    "    logging.info(\"Only one or zero PDFs found, not processing a second one.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
